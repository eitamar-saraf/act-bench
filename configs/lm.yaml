# Default configuration for the language modeling task (GPT on WikiText-2)
task: "lm"
activation: "relu"

learning_rate: 1e-4
batch_size: 16
max_epochs: 10
warmup_steps: 200
adamw_weight_decay: 0.01
max_len: 256
